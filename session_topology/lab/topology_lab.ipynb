{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rui Benfeitas, Scilifelab, NBIS National Bioinformatics Infrastructure Sweden   </h3>\n",
    "rui.benfeitas@scilifelab.se\n",
    "\n",
    "**Abstract**  \n",
    "In this notebook we will explore how to generate and analyse a multi-omic network comprising metabolites quantifications and gene expression. We will compare these networks against randomly generated networks, and compute different network metrics. At the end we will also perform a community analysis and functional characterization at the gene level.   \n",
    "<br>\n",
    "<br>\n",
    "**Contents**\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Biological-network-topology-analysis\" data-toc-modified-id=\"Biological-network-topology-analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Biological network topology analysis</a></span></li><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href=\"#Association-analysis\" data-toc-modified-id=\"Association-analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Association analysis</a></span></li><li><span><a href=\"#Network-construction-and-preliminary-analysis\" data-toc-modified-id=\"Network-construction-and-preliminary-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Network construction and preliminary analysis</a></span></li><li><span><a href=\"#Centrality-analysis\" data-toc-modified-id=\"Centrality-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Centrality analysis</a></span></li><li><span><a href=\"#Community-detection\" data-toc-modified-id=\"Community-detection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Community detection</a></span></li><li><span><a href=\"#Extracting-communities\" data-toc-modified-id=\"Extracting-communities-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Extracting communities</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install cairocffi -y\n",
    "!conda install anaconda::pycairo -y\n",
    "!conda install conda-forge::leidenalg -y    \n",
    "# After intallation, restart the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preamble\n",
    "import sklearn, itertools, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import sklearn.neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy as sp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import gseapy as gp\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import leidenalg \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological network topology analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**  \n",
    "In this notebook you will learn how to build and analyse a network built and analysed from a gene-metabolite association analysis. Other mixed networks may also be similarly analyzed, differring only in whether and how you can apply the final functional analysis.\n",
    "\n",
    "**Data**  \n",
    "As a test case we will be using the file [met_genes.tsv](met_genes.tsv) which contains abundances for 125 metabolites and 1992 genes, for 24 samples.\n",
    "\n",
    "**Software**  \n",
    "This notebook relies on python's igraph for most of the analyses. Most, if not all, functions used here can also be applied with R's igraph. Other packages exist for network analysis including [networkx](https://networkx.github.io/) and [graph-tool](https://graph-tool.skewed.de/). [Snap.py](https://snap.stanford.edu/snappy/) is also a good alternative for large networks.\n",
    "\n",
    "We will build our network through an association analysis, but there are other methods to do this including [Graphical Lasso](http://statweb.stanford.edu/~tibs/ftp/graph.pdf) or [linear SVR](https://papers.nips.cc/paper/1187-support-vector-method-for-function-approximation-regression-estimation-and-signal-processing.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a gene expression dataset (RNAseq, expressed as TPM) from a disease group with 24 samples to keep analysis memory and time requirements reasonable for this lesson. It is assumed that all batch effects or other possible technical artifacts are not present, and that all data is ready for analysis. However, there are several important considerations in preprocessing your data:\n",
    " - How should you deal with missing values? Should you impute them? How?\n",
    " - Should you remove samples based on number of missing values?\n",
    " - How should you normalize your data in order to make it comparable throughout?\n",
    " \n",
    "This will depend on the type of that that you have and what you want to do with it, and will severely affect downstream results. It is thus important to carefully think about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/met_genes.tsv', sep=\"\\t\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicated features are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(data.index.duplicated()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby('Type').agg('count')[['p10']] #1992 genes, 125 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape #2117 features, 25 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very quick view shows that several gene clusters are found, including two major groups. However, the analysis below does not perform any statistical filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial network analysis will be performed on the association analysis using Spearman correlations. Because this network has a big chance of producing false positives we will consider [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) to control for familywise error, as well as [FDR](https://en.wikipedia.org/wiki/False_discovery_rate#Benjamini%E2%80%93Hochberg_procedure).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very quick view shows that several gene clusters are found, including two major groups. However, the analysis below does not perform any statistical filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=data.loc[:,data.columns!='Type']\n",
    "meta=data[['Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.isna().any().any() #we have no rows with NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a gene-gene, gene-metabolite, and metabolite-metabolite association analysis by computing pairwise [Spearman correlations](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr). Choosing other non-parametric (Kendall or Spearman) vs parametric (Pearson) methods depends on your data. Here, because we have a small sample size, and want to save on computational time we choose Spearman. \n",
    "\n",
    "The following takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation and P val matrices\n",
    "Rmatrix, Pmatrix= sp.stats.spearmanr(values.T)\n",
    "Rmatrix=pd.DataFrame(Rmatrix, index=values.index.copy(), columns=values.index.copy())\n",
    "\n",
    "#resulting R matrix\n",
    "sns.clustermap(Rmatrix, cmap=\"RdBu_r\", center=0); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the matrix of P values, we can already see that many of the correlations in the top right columns are not significant even before multiple hypothesis correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pmatrix=pd.DataFrame(Pmatrix, index=values.index.copy(), columns=values.index.copy())\n",
    "changed_Pmatrix=Pmatrix.copy()\n",
    "changed_Pmatrix[changed_Pmatrix>0.01]=1\n",
    "#resulting P matrix, similar to that seen in the section above\n",
    "sns.clustermap(changed_Pmatrix); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now adjust the P values based on the number of comparisons done. The heatmaps above are highlighting a total of $2117^2 \\approx 4.5m$ correlations. However, these numbers consider that the same correlation is computed twice (gene A vs gene B, and gene B vs gene A). If we were to include all correlations, we would thus be including many repeated analyses, and we are only interested in half of that above, and excluding the correlation of a feature with itself.   \n",
    "This means $\\frac{2117!}{2!(2117-2)!} \\approx 2.2m$ correlations. At an error rate of 0.05, this means that the probability of finding at least one false positive is nearly 100%: $1-0.95^{2000000} \\approx 1$. We thus need to correct P values.\n",
    "\n",
    "In the following cell, we convert the matrix of p*p features to a long matrix, concatenate both R and P for each correlation, and correct based on [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction) (`Padj`) and [FDR](https://en.wikipedia.org/wiki/False_discovery_rate) (Benjamin-Hochberg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare P matrix\n",
    "Psquared=Pmatrix.where(np.triu(np.ones(Pmatrix.shape),1).astype(bool))\n",
    "Psquared.columns.name='Feat2'\n",
    "Pmatrix=Pmatrix.stack()\n",
    "Pmatrix.index.names=['v1','v2']\n",
    "Pmatrix=Pmatrix.reset_index()\n",
    "Pmatrix.columns=['feat1','feat2','P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare R matrix\n",
    "Rmatrix=Rmatrix.where(np.triu(np.ones(Rmatrix.shape),1).astype(bool))\n",
    "Rmatrix.columns.name='Feat2'\n",
    "Rmatrix=Rmatrix.stack()\n",
    "Rmatrix.index.names=['v1','v2'] #Avoid stacked names colliding\n",
    "Rmatrix=Rmatrix.reset_index()\n",
    "Rmatrix.columns=['feat1','feat2','R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both\n",
    "PRmatrix=pd.merge(Rmatrix.copy(), Pmatrix.copy(), on=['feat1','feat2']) #Correlation matrix with both R and P\n",
    "PRmatrix=PRmatrix.loc[PRmatrix.feat1!=PRmatrix.feat2].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple hypothesis correction computed on the P column\n",
    "adjP=pd.DataFrame(multipletests(PRmatrix['P'], method='bonferroni', alpha=0.01)[1], columns=['Padj'])\n",
    "FDR=pd.DataFrame(multipletests(PRmatrix['P'], method='fdr_bh', alpha=0.01)[1], columns=['FDR'])\n",
    "\n",
    "PRmatrix=pd.concat([ PRmatrix, adjP], axis=1)\n",
    "PRmatrix=pd.concat([ PRmatrix, FDR], axis=1)\n",
    "\n",
    "PRmatrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of correlations w/o repetition: 2.2m\n",
    "PRmatrix.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Bonferroni correction we find 16305 correlations that are statistically significant at an $\\alpha < 0.01$. If we consider instead FDR as correction method, we find 402368, which at a FDR of 0.01 implies $0.01 \\times 402368 = 4023$ false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(PRmatrix.Padj<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(PRmatrix.FDR<0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add two additional columns, where we assign `R=0` for those correlations that are not statistically significant (`adjP > 0.01`, and `FDR > 0.01`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRmatrix.loc[:,'R (padj)']=PRmatrix['R'].copy()\n",
    "PRmatrix.loc[:,'R (fdr)']=PRmatrix['R'].copy()\n",
    "PRmatrix.loc[PRmatrix['Padj']>0.01,'R (padj)']=0\n",
    "PRmatrix.loc[PRmatrix['FDR']>0.01,'R (fdr)']=0\n",
    "\n",
    "all_mets=meta.loc[meta.Type=='met'].index\n",
    "PRmatrix['feat1_type']=['met' if x in all_mets else 'gene' for x in PRmatrix.feat1 ]\n",
    "PRmatrix['feat2_type']=['met' if x in all_mets else 'gene' for x in PRmatrix.feat2 ]\n",
    "PRmatrix['int_type']=PRmatrix.feat1_type+'_'+PRmatrix.feat2_type\n",
    "PRmatrix.to_csv('data/association_matrix.tsv', sep=\"\\t\", index=False) #export correlation matrix for faster loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how the initial heatmap of correlations looks. The next cell converts the matrix from long to squared matrix so that we can generate the heatmap. We will plot those features that show statistically significant associations with more than 5% of the features after FDR correction, and compare them between FDR- and Bonferroni-corrected datasets. \n",
    "\n",
    "The following plot shows the heatmap of the Spearman rank correlation coefficients after Bonferroni-correction - correlations where Padj > 0.05 are shown as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Transforming to a squared matrix again\n",
    "PRQ=pd.concat([\n",
    "    PRmatrix.copy(), \n",
    "    PRmatrix.copy().rename(columns={'feat1':'feat2','feat2':'feat1'}).loc[:,PRmatrix.columns]\n",
    "         ]).drop_duplicates()\n",
    "\n",
    "Rmatrix_fdr=PRQ.copy().pivot(index='feat1',columns='feat2',values='R (fdr)')\n",
    "Rmatrix_fdr=Rmatrix_fdr.loc[Rmatrix_fdr.sum()!=0]\n",
    "Rmatrix_padj=PRQ.copy().pivot(index='feat1',columns='feat2',values='R (padj)')\n",
    "\n",
    "Rmatrix_fdr=Rmatrix_fdr.loc[Rmatrix_fdr.index,Rmatrix_fdr.index].fillna(0)\n",
    "Rmatrix_padj=Rmatrix_padj.loc[Rmatrix_fdr.index,Rmatrix_fdr.index].fillna(0)\n",
    "\n",
    "\n",
    "#Showing only the top correlated features\n",
    "top_features=Rmatrix_fdr.index[(Rmatrix_fdr!=0).sum()>0.05*Rmatrix_fdr.shape[0]] #top features based on FDR\n",
    "Rmatrix_fdr_top=Rmatrix_fdr.copy().loc[top_features,top_features] #subsetting R (fdr corrected) matrix\n",
    "Rmatrix_padj_top=Rmatrix_padj.copy().loc[top_features,top_features] #subsetting R (bonferroni corrected) matrix\n",
    "\n",
    "g=sns.clustermap(Rmatrix_padj_top, cmap=\"RdBu_r\", center=0);\n",
    "g.fig.suptitle('Spearman R (Padj < 0.01, Bonferroni)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.clustermap(Rmatrix_fdr_top, cmap=\"RdBu_r\", center=0);\n",
    "g.fig.suptitle('Spearman R (FDR<0.01)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show that the Bonferroni correction is only selecting very high (absolute) correlations. This should remove false positives, but it may also remove weaker correlations that are biologically relevant and true positives. The Bonferroni correction also removes most of the negatively-associated features. Notice this from the distribution of correlation coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortPR=PRmatrix.copy().loc[:,['feat1','feat2','R (padj)','R (fdr)']]\n",
    "shortPR=shortPR.loc[shortPR.feat1!=shortPR.feat2]\n",
    "\n",
    "fig=plt.figure(figsize=(8,4))\n",
    "p=sns.histplot(shortPR['R (padj)'][shortPR['R (padj)']!=0], color='black', label='Bonferroni (<0.01)', kde=True, bins=100);\n",
    "p.set(ylabel='PDF (Bonferroni)')\n",
    "ax2=p.twinx()\n",
    "g=sns.histplot(shortPR['R (fdr)'][shortPR['R (fdr)']!=0], ax=ax2, color='red', label='FDR (<0.01)', kde=True, bins=100);\n",
    "g.set(ylabel='PDF (FDR)')\n",
    "\n",
    "fig.legend()\n",
    "plt.xlabel('R')\n",
    "plt.title('Distribution of correlation coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that the Bonferroni correction yields a more homogeneous number of associated features for each feature (i.e. first neighbors), compared to the FDR filtering. This has a consequence on the network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_number=pd.DataFrame()\n",
    "for n_neighbors in np.arange(1,21):\n",
    "    padj_count=((Rmatrix_padj!=0).sum()==n_neighbors).sum()\n",
    "    fdr_count=((Rmatrix_fdr!=0).sum()==n_neighbors).sum()\n",
    "    \n",
    "    out=pd.Series([padj_count, fdr_count], index=['num_features(Padj)','num_features(FDR)'], name=n_neighbors)\n",
    "    neighbor_number=pd.concat([neighbor_number, out], axis=1)\n",
    "    \n",
    "neighbor_number=neighbor_number.T.rename_axis('num_neighbors').reset_index()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(16, 8), nrows=2)\n",
    "ax=ax.flatten()\n",
    "sns.lineplot(data=neighbor_number, x='num_neighbors', y='num_features(Padj)', color='black', label='Padj', ax=ax[0]);\n",
    "sns.lineplot(data=neighbor_number, x='num_neighbors', y='num_features(FDR)', color='red', label='FDR', ax=ax[0]);\n",
    "ax[0].set(ylabel='Number of features')\n",
    "fig.suptitle('Number of features with <21 neighbors')\n",
    "ax[0].set_xticks(np.arange(1,21,1));\n",
    "\n",
    "### boxplot requires a long df\n",
    "feat_associations_padj=pd.concat([\n",
    "    shortPR.copy().loc[shortPR['R (padj)']!=0,][['feat1','feat2']],\n",
    "    shortPR.copy().loc[shortPR['R (padj)']!=0,][['feat2','feat1']].rename(columns={'feat1':'feat2','feat2':'feat1'})]).drop_duplicates().groupby('feat1').agg('count')\n",
    "\n",
    "feat_associations_fdr=pd.concat([\n",
    "    shortPR.copy().loc[shortPR['R (fdr)']!=0,][['feat1','feat2']],\n",
    "    shortPR.copy().loc[shortPR['R (fdr)']!=0,][['feat2','feat1']].rename(columns={'feat1':'feat2','feat2':'feat1'})]).drop_duplicates().groupby('feat1').agg('count')\n",
    "\n",
    "feat_associations=pd.concat([feat_associations_padj, feat_associations_fdr], axis=1, sort=True)\n",
    "feat_associations.fillna(0,inplace=True)\n",
    "feat_associations.columns=['Padj','FDR']\n",
    "\n",
    "sns.boxplot( data=feat_associations, notch=True, ax=ax[1], orient='h', palette={'Padj':'black','FDR':'red'});\n",
    "plt.title('Number of associations per feature')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Bonferroni correction leads to many nodes being associated with 1 or 2 other nodes, whereas the FDR correction leads to substantially higher number of associations for some of the nodes. This can also raise questions about the biological plausibility of such high number of associations - is it biologically significant that a gene is co-expressed with 800 other genes/metabolites?\n",
    "\n",
    "We can also be a bit more strict on the FDR that we consider as statistically significant. Let's compare the number of potential false positives at different FDR. The following plot further highlights an FDR = 0.01 (dashed gray line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_df=pd.DataFrame()\n",
    "for fdr in np.append(np.geomspace(1e-4, 0.01, 30), [0.02, 0.03, 0.04, 0.05,0.1]):\n",
    "    out=pd.Series([fdr,(PRmatrix.Padj<fdr).sum(),np.round(fdr*((PRmatrix.Padj<fdr).sum()),2)],\n",
    "                  index=['FDR','num_edges','num_fp'])\n",
    "    fdr_df=pd.concat([fdr_df, out], axis=1)\n",
    "fdr_df=fdr_df.T\n",
    "    \n",
    "fig,ax=plt.subplots(figsize=(10,4))\n",
    "sns.lineplot(data=fdr_df, x='FDR', y='num_edges', color='black', ax=ax, label='Edge number')\n",
    "sns.lineplot(data=fdr_df, x='FDR', y='num_fp', color='red', ax=ax, label='# potential false positives')\n",
    "ax.set(xscale='log', yscale='log', ylabel='Edge #');\n",
    "ax.set_xticks([1e-4,1e-3, 1e-2, 1e-1, 0.05])\n",
    "ax.set_xticklabels([1e-4,1e-3, 1e-2, 1e-1, 0.05])\n",
    "ax.axvline(0.01, dashes=[0,0,5,5], color='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the number of potential false positives increases substantially with the number of edges, showing that networks with large sizes can have a high number of false positives. Thus, selecting an appropriate cut-off needs to take into account these two quantities.  \n",
    "By being more conservative, one will be more sure of the identified associations but a network that is too small may miss biologically important associations (i.e. have many false negatives) and become too sparse to be representative of the biology. This may result in very small communities of nodes, and many dyads and isolated vertices. On the other hand, a network that is too big may display a very high number of associations that are not observed biologically (false positives). It is likely that such networks will display large communities and high density. Think of the balance between [Type I and Type II errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error). Unfortunately, because this is an unsupervised problem we have no way to compute the false negative rate. This is not always the case: if you are dealing with human proteomics, one possible solution to identify false negatives can be from using a [reference map of human protein-protein interactions](https://www.nature.com/articles/s41586-020-2188-x).\n",
    "\n",
    "In choosing an appropriate cut-off we will thus rely on the number of potential false positives and the network dimensions. In another section below, we will compare different networks and explore how their structure differs both in network properties (e.g. [centrality](https://en.wikipedia.org/wiki/Centrality)) and [communities](https://en.wikipedia.org/wiki/Community_structure). Bear in mind that false positives should not have an extensive impact on the network communities if these spurious associations are randomly distributed throughout the network, since the community interrogation algorithms usually compute regions of high density. \n",
    "\n",
    "We will also look at how the statistically significant correlation coefficients change FDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,4))\n",
    "\n",
    "for fdr in [0.05, 0.01, 0.001, 0.0001]:\n",
    "    ## Plotting R distribution\n",
    "    temp=PRmatrix.copy()\n",
    "    temp.loc[:,'R (fdr)']=temp['R'].copy()\n",
    "    temp.loc[temp['FDR']>fdr,'R (fdr)']=0\n",
    "    \n",
    "    temp=temp.loc[temp.feat1!=shortPR.feat2]\n",
    "    \n",
    "    color={0.05:'gray',0.01:'green',0.001:'red',0.0001:'blue'}[fdr]\n",
    "\n",
    "    \n",
    "    p=sns.histplot(\n",
    "        temp['R (fdr)'][temp['R (fdr)']!=0], color=color, label='FDR <'+str(fdr), kde=True, ax=ax, bins=50);\n",
    "    p.set(ylabel='PDF (FDR)')\n",
    "\n",
    "fig.legend()\n",
    "plt.xlabel('R (FDR)');\n",
    "plt.title('Distribution of correlation coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicably, by being more conservative we will be selecting higher absolute correlation coefficients. In this test case we are considering only 25 samples. Larger sample sizes lead to lower nominal and adjusted p-values, and a higher number of statistically significant but milder correlation coefficients. In such cases, one may be more conservative in the significance threshold. Henceforth, we will consider as statistically significant those edges where FDR < 0.01.\n",
    "\n",
    "One last point comes from the comparison of statistically significant associations within and between omics. Note how so few inter-omic associations are identified, and that Bonferroni correction completely misses any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonferroni_significant=PRmatrix.copy().loc[PRmatrix.Padj<0.01].loc[:,['feat1','feat2','R','int_type']]\n",
    "bonferroni_significant['sig_test']='bonferroni'\n",
    "fdr_significant=PRmatrix.copy().loc[PRmatrix.FDR<0.01].loc[:,['feat1','feat2','R','int_type']]\n",
    "fdr_significant['sig_test']='FDR'\n",
    "\n",
    "pd.concat([bonferroni_significant,fdr_significant]).groupby(['sig_test','int_type'])['R'].agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "In building a graph from an association analysis:\n",
    "- Why do you think that most significant correlations are found only within each omic? \n",
    "- How will you deal with the positive and negative sets of correlations above?\n",
    "- Will you consider the network as weighted? Directed?\n",
    "- Which dataset would you select for further analysis: the Bonferroni or the FDR-corrected? Why?\n",
    "- What preliminary tests would you perform on the graph to assess whether node relationships are random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network construction and preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build 4 different networks to analyse further:\n",
    "- A full association network filtered using FDR-corrected P values (<0.01). This is an unweighted network.\n",
    "- The subset of positively associated features, where correlation coefficient is used as weight.\n",
    "- kNN-G that we will generate from the expression profile. This will be unweighted.\n",
    "- A random network based on the [Erdos-Renyi model](https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model), with the same node and edge number of each network. \n",
    "\n",
    "This will be a null-model for our analyses. The idea is that if a certain property found in one of our graphs is  reproduced in a random graph, then we do not need to account for any other possible explanations for that feature. In other words, if a property of a graph (e.g. clustering) is not found in a random network, we can assume that it does not appear in our biological network due to randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares table for being read by igraph\n",
    "PRmatrix=pd.read_csv('data/association_matrix.tsv', sep=\"\\t\")\n",
    "PRmatrix.loc[PRmatrix['FDR']>0.01,'R (fdr)']=0\n",
    "PRmatrix=PRmatrix.loc[PRmatrix['R (fdr)']!=0,['feat1','feat2','R (fdr)']]\n",
    "PRmatrix=PRmatrix.loc[PRmatrix.feat1!=PRmatrix.feat2] #drops self correlations\n",
    "\n",
    "fdr_pos_mat=PRmatrix.loc[PRmatrix['R (fdr)']>0]\n",
    "fdr_neg_mat=PRmatrix.loc[PRmatrix['R (fdr)']<0]\n",
    "\n",
    "PRmatrix=PRmatrix.loc[PRmatrix.isin(pd.unique(fdr_pos_mat[['feat1','feat2']].values.flatten())).sum(1)==2,] #selects only nodes also found in the positive network so that we can compare networks of the same sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the kNNG, using distances as input to determine the nearest neighbours. Because this data contains both gene expressions and metabolite quantifications, we need to normalize them beforehand. (We didn't need to do this above as we were comparing ranks)\n",
    "\n",
    "We start by standardizing all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and normalizes met and gene data so that we can compute similarities between them\n",
    "data=pd.read_csv('data/met_genes.tsv', sep=\"\\t\", index_col=0)\n",
    "\n",
    "scaled_data=pd.DataFrame(StandardScaler().fit_transform(data.loc[:,data.columns!='Type'].T).T, columns=data.columns[data.columns!='Type'], index=data.index)\n",
    "scaled_data_values=scaled_data.copy()\n",
    "scaled_data['Type']=data.Type\n",
    "\n",
    "#Plots the data distribution\n",
    "fig,ax=plt.subplots(figsize=(12,8))\n",
    "sns.kdeplot(\n",
    "    scaled_data.loc[scaled_data.Type=='met',scaled_data.columns!='Type'].values.flatten(), \n",
    "    color='r', label='Mets', ax=ax, legend=False)\n",
    "ax2=ax.twinx()\n",
    "sns.kdeplot(\n",
    "    scaled_data.loc[scaled_data.Type=='genes',scaled_data.columns!='Type'].values.flatten(),\n",
    "    color='b', label='Genes', ax=ax2, legend=False)\n",
    "fig.suptitle('Distribution for met and genes');\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate the graphs from the dataframes above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the kNN graph\n",
    "#Computes a kNN adjacency matrix from the input dataset\n",
    "#and prepares table for being read by igraph\n",
    "input_ds=scaled_data_values.loc[scaled_data_values.index.isin(pd.unique(fdr_pos_mat[['feat1','feat2']].values.flatten()))]\n",
    "knnG=sklearn.neighbors.kneighbors_graph(input_ds.values, 200, metric='euclidean')\n",
    "knnG=pd.DataFrame(knnG.toarray(), columns=input_ds.index.copy(), index=input_ds.index.copy())  #adjacency matrix\n",
    "knnG.index.name='gene1'\n",
    "knnG.columns.name='gene2'\n",
    "knnG=knnG.stack().reset_index().rename(columns={0:'Connectivity'})\n",
    "knnG=knnG.loc[knnG['Connectivity']!=0]\n",
    "\n",
    "### Generates each of the graphs\n",
    "#positive associations, weighted\n",
    "pos_w=ig.Graph.TupleList([tuple(x) for x in fdr_pos_mat.values], directed=False, edge_attrs=['w'])\n",
    "\n",
    "#full network, unweighted\n",
    "edge_list=PRmatrix.copy().loc[PRmatrix.isin(pd.unique(fdr_pos_mat[['feat1','feat2']].values.flatten())).sum(1)==2,['feat1','feat2']].values\n",
    "all_u=ig.Graph.TupleList([tuple(x) for x in edge_list], directed=False)\n",
    "\n",
    "#knnG, unweighted\n",
    "knn=ig.Graph.TupleList([tuple(x) for x in knnG.values], directed=False)\n",
    "\n",
    "#random network, unweighted, node and edge number based on a network of the same size\n",
    "random_posw=ig.Graph.Erdos_Renyi(n=input_ds.shape[0], m=len(fdr_pos_mat.values), directed=False, loops=False)\n",
    "\n",
    "random_allu=ig.Graph.Erdos_Renyi(n=input_ds.shape[0], m=len(edge_list), directed=False, loops=False)\n",
    "\n",
    "random_knn=ig.Graph.Erdos_Renyi(n=input_ds.shape[0], m=len(knnG.values), directed=False, loops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For representation purposes we will see how a short knn graph looks - be careful in drawing the others, as they have many more edges it becomes computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#random subset of knn graph for plotting\n",
    "short_knn=ig.Graph.TupleList([tuple(x) for x in knnG.values[random.sample(list(np.arange(len(knnG.values))), 5000)]], directed=False)\n",
    "\n",
    "#This plots each graph, using degree to present node size:\n",
    "short_knn.vs['degree']=short_knn.degree() \n",
    "short_knn.vs['degree_size']=[(x*15)/(max(short_knn.vs['degree'])) for x in short_knn.vs['degree']] #degree is multiplied by 10 so that we can see all nodes\n",
    "\n",
    "layout = short_knn.layout_mds()\n",
    "ig.plot(short_knn, layout=layout, vertex_color='white', edge_color='silver', vertex_size=short_knn.vs['degree_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next table, we can see that while the same number of nodes is found in all networks, the number of edges varies greatly. We also see that the network is fully connected, which is not allways the case. If it wasn't connected, we could select the ***k***  largest connected components, and proceed the analyses with them. The largest connected component is called the *giant component*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get graph properties, takes a few minutes to run\n",
    "def graph_prop(input_graph):\n",
    "    ncount=nn.vcount()\n",
    "    ecount=nn.ecount()\n",
    "    diameter=nn.diameter()\n",
    "    av_path=nn.average_path_length()\n",
    "    dens=nn.density()\n",
    "    clustering=nn.transitivity_undirected() #this is the global clustering coefficient\n",
    "    conn=nn.is_connected()\n",
    "    min_cut=nn.mincut_value()\n",
    "    out=pd.DataFrame([ncount, ecount, diameter, av_path, dens, clustering, conn, min_cut],\n",
    "                 index=['node_count','edge_count','diameter','av_path_length','density','clustering_coef','connected?','minimum_cut']).T\n",
    "    return(out)\n",
    "\n",
    "#summarizing graph properties\n",
    "network_stats=pd.DataFrame()\n",
    "for nn in [pos_w, all_u, knn, random_posw, random_allu, random_knn]:\n",
    "    network_stats=pd.concat([network_stats,graph_prop(nn)])\n",
    "    \n",
    "network_stats.index=['pos_w','all_u','knn','pos_w_random', 'all_u_random', 'knn_random']\n",
    "network_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Why is the diameter and average path length lower in the case of the full network and the random network, compared to the other two networks? What about the other random networks?\n",
    "- Why do you think the clustering coefficient is lower for the knn compared with the other networks?\n",
    "- Why is the minimum cut much larger in the random network compared to the others?\n",
    "- How do you think the selected *k* would influence the properties above for kNNG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look into different centrality measures:\n",
    "- [Degree](https://en.wikipedia.org/wiki/Degree_(graph_theory)) - number of neighbors of a node\n",
    "- [Betweenness](https://en.wikipedia.org/wiki/Betweenness_centrality) - measures how many shortest paths in the network pass through a node.\n",
    "- [Closeness](https://en.wikipedia.org/wiki/Centrality#Closeness_centrality) - the average length of the shortest paths between a node and all other nodes \n",
    "- [Eccentricity](https://en.wikipedia.org/wiki/Distance_(graph_theory)) - largest shortest path from a node to any other node. Nodes with high eccentricity tend to be on the periphery.\n",
    "- [Eigenvector centrality](https://en.wikipedia.org/wiki/Eigenvector_centrality) - a node is more central if its neighbors show a high degree.\n",
    "\n",
    "Degree, Betweenness, Closeness and Eigenvector centralities may be additionally computed for the positive association network by taking into account each edge's weight. For instance, for degree this is done for each node by summing each edge's degree.\n",
    "\n",
    "Because the number of shortest paths in a network scales with the network size, we normalize Eccentricity and Betweenness with respect to the network size so that they can be compared between the four networks above.\n",
    "\n",
    "Note that many [other centrality metrics](https://en.wikipedia.org/wiki/Centrality) can be computed. For instance, [PageRank](https://en.wikipedia.org/wiki/PageRank) and [HITS](https://en.wikipedia.org/wiki/HITS_algorithm) take into account edge directionality to compute what are the most central nodes in a network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Degree distribution**  \n",
    "Let's start by comparing the degrees of the random network against the three other networks. From the figures below it seems that there is no relationship between the degree of the random network, and any of the three others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i, ax in zip(range(4), axes.flat):\n",
    "    sns.regplot(x=[pos_w,all_u,knn][i].degree(), y=[random_posw, random_allu, random_knn][i].degree(), \n",
    "                ax=ax, color=['blue','lightsalmon','green'][i],\n",
    "               line_kws={'color':'black'}\n",
    "               );\n",
    "    ax.set_title(['pos','all','knn'][i])\n",
    "    ax.set(xlabel='k ('+['pos','all','knn'][i]+')', ylabel='k (random)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def transform_degree(graph):\n",
    "    alldegs=graph.degree()\n",
    "    alldegs=pd.DataFrame([[key,len(list(group))] for key,group in itertools.groupby(alldegs)], columns=['k','count'])\n",
    "    alldegs['P(k)']=[x/alldegs['count'].sum() for x in alldegs['count']]\n",
    "    alldegs=alldegs.loc[:,['k','P(k)']]\n",
    "    alldegs.drop_duplicates(inplace=True)\n",
    "    alldegs.reset_index(drop=True, inplace=True)\n",
    "    return(alldegs)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# ax.set(yscale='log', xscale='log')\n",
    "p=sp.stats.probplot(pos_w.degree(), plot=ax)\n",
    "a=sp.stats.probplot(all_u.degree(), plot=ax)\n",
    "k=sp.stats.probplot(knn.degree(), plot=ax)\n",
    "r=sp.stats.probplot(random_posw.degree(), plot=ax)\n",
    "r2=sp.stats.probplot(random_allu.degree(), plot=ax)\n",
    "r3=sp.stats.probplot(random_knn.degree(), plot=ax)\n",
    "\n",
    "col=['blue','','peru','','green','','lightblue','','lightsalmon','','greenyellow','']\n",
    "for x in np.arange(0,11,2):\n",
    "    ax.get_lines()[x].set_markerfacecolor(col[x])\n",
    "    ax.get_lines()[x].set_markeredgewidth(0)\n",
    "    ax.get_lines()[x+1].set_color(col[x])\n",
    "\n",
    "\n",
    "fig.legend(labels=['pos','pos','all','all','knn','knn','pos_random','pos_random','all_random','all_random','knn_random','knn_random']);\n",
    "\n",
    "ax.set(xlabel='Data quantiles', ylabel='observed degree (k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centrality**  \n",
    "We will now compare different centrality metrics between the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_list=['degree (larger ~ central)','eccentricity (smaller ~ central)','betweenness (larger ~ central)', 'closeness (larger ~ central)','eigenvector (larger ~ central)']\n",
    "\n",
    "def combine_raw_centralities():\n",
    "    \"\"\"\n",
    "    Combines centrality metrics\n",
    "    \"\"\"\n",
    "    def centrality_raw(input_graph, graph_name):\n",
    "        \"\"\"\n",
    "        Computes centrality metrics for a graph.\n",
    "        \"\"\"\n",
    "        deg=input_graph.degree(loops=False)\n",
    "        node_n=input_graph.vcount()\n",
    "        #scaled to account for network size\n",
    "        ecc=[(2*x / ((node_n-1)*(node_n-2))) for x in input_graph.eccentricity()]\n",
    "        btw=[(2*x / ((node_n-1)*(node_n-2))) for x in input_graph.betweenness(directed=False)] \n",
    "        eig=input_graph.eigenvector_centrality(directed=False, scale=False)\n",
    "        \n",
    "        # For disconnected graphs, computes closeness from the largest connected component\n",
    "        if(input_graph.is_connected()):\n",
    "            cls=input_graph.closeness(normalized=True)\n",
    "        else:\n",
    "            cls=input_graph.clusters(mode='WEAK').giant().closeness(normalized=True)\n",
    "        \n",
    "        out=pd.DataFrame([deg, ecc, btw, cls,eig], index=centralities_list).T\n",
    "        out['graph']=graph_name\n",
    "        out=out.loc[:,np.append(['graph'],out.columns[out.columns!='graph'])]\n",
    "        \n",
    "        ##Adds centralities for each node in the network\n",
    "        input_graph.vs['degree']=out['degree (larger ~ central)']\n",
    "        input_graph.vs['eccentricity']=out['eccentricity (smaller ~ central)']\n",
    "        input_graph.vs['betweenness']=out['betweenness (larger ~ central)']\n",
    "        input_graph.vs['closeness']=out['closeness (larger ~ central)']\n",
    "        input_graph.vs['eigenvector']=out['eigenvector (larger ~ central)']\n",
    "        \n",
    "        return(out)\n",
    "    \n",
    "    #Computes centralities for all networks\n",
    "    network_centralities_raw=pd.DataFrame()\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(pos_w,'pos_w')])\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(all_u,'all_u')])\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(knn,'knn')])\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(random_posw,'pos_w_random')])\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(random_allu,'all_u_random')])\n",
    "    network_centralities_raw=pd.concat([network_centralities_raw,centrality_raw(random_knn,'knn_random')])\n",
    "    return(network_centralities_raw)\n",
    "\n",
    "network_centralities_raw=combine_raw_centralities()\n",
    "network_centralities_raw.index= range(network_centralities_raw.shape[0])\n",
    "\n",
    "\n",
    "fig, axes=plt.subplots(nrows=5, figsize=(16,20), sharey='row')\n",
    "for i, ax in zip(range(6), axes.flat):\n",
    "    sns.boxplot(\n",
    "        data=network_centralities_raw, x='graph', y=centralities_list[i],  notch=True, ax=ax, showfliers=False)\n",
    "    ax.set_title(centralities_list[i])\n",
    "    ax.set(xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting the results above, it is important to bear in mind that these networks have different network sizes. Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we see:\n",
    "- The median degree centrality decreases from `Full network > kNNG >  Positive assoc. network`.\n",
    "- The median betweenness centrality tends to decrease from `Positive assoc. network > Full network > kNNG`.\n",
    "- The median closeness centrality  tends to decrease from `Full network > kNNG > Positive assoc. network`.\n",
    "- The eccentricity is very homogeneous for the `Full network (all_u)`, and slightly lower than in the `Full network`. In turn, most nodes in the `kNNG` tend to display an eccentricity of 3-4.\n",
    "\n",
    "\n",
    "### Questions:\n",
    "- Can you explain these observations?\n",
    "- Based on the plots above, which graphs do you think follow a [*small world*](https://en.wikipedia.org/wiki/Small-world_network) behavior?\n",
    "\n",
    "We will also explore the relationships between different centrality metrics. Because these have different interpretations, we will compute ranks for each centrality, and perform the correlations on the ranks. In the following cell we do this, and then compute correlations within the 5 metrics for the full network. One additional column is presented (`median_centrality`), that is basically the median of the ranks of the 5 other centralities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute centrality ranks so that we can compare them within and between networks\n",
    "full_centralities=pd.DataFrame()\n",
    "for net in [0,1,2]:\n",
    "    net_in=[pos_w, all_u, knn][net]\n",
    "    net_nm=['pos_w', 'all_u', 'knn'][net]\n",
    "    temp=pd.DataFrame([net_in.vs[att] for att in ['name','degree','betweenness', 'closeness','eccentricity','eigenvector']], index=['name','degree','betweenness', 'closeness','eccentricity','eigenvector']).T\n",
    "    temp.columns=[x+'|'+net_nm for x in temp.columns]\n",
    "    temp.rename(columns={'name|'+net_nm:'name'}, inplace=True)\n",
    "    \n",
    "    ## For all but eccentricity centrality, we compute the rank in ascending mode\n",
    "    ## so that higher ranking means more central. we need to reverse this for eccentricity\n",
    "    temp.loc[:,temp.columns.str.contains('deg|bet|clos|eig')]=temp.loc[:,temp.columns.str.contains('deg|bet|clos|eig')].rank(pct=True, ascending=True)\n",
    "    temp.loc[:,temp.columns.str.contains('eccentricity')]=temp.loc[:,temp.columns.str.contains('eccentricity')].rank(pct=True, ascending=False\n",
    "                                                              )\n",
    "    temp['median_centrality|'+net_nm]=temp.loc[:,temp.columns!='name'].median(1)\n",
    "    if(net==0):\n",
    "        full_centralities=temp\n",
    "    else:\n",
    "        full_centralities=pd.merge(full_centralities, temp, on='name')\n",
    "full_centralities.set_index('name', inplace=True)\n",
    "full_centralities=pd.merge(full_centralities, data[['Type']], left_index=True, right_index=True, how='left')\n",
    "full_centralities['median|ALL']=full_centralities.loc[:,full_centralities.columns.str.contains('median')].median(1)\n",
    "\n",
    "\n",
    "### Correlations are computed between ranks, after inverting the rank for eccentricity\n",
    "def correlations_centralities(graph_name):\n",
    "    \"\"\"\n",
    "    Returns squared correlation matrix.\n",
    "    \"\"\"\n",
    "    temp_corr=full_centralities.copy().loc[:,full_centralities.columns!='Type'].dropna().astype('float')\n",
    "    temp_corr=temp_corr.loc[:,temp_corr.columns.str.contains(graph_name)]\n",
    "    temp_corr.columns=temp_corr.columns.str.replace('\\|.+','')\n",
    "    temp_corr=temp_corr.corr(method='spearman')\n",
    "    np.fill_diagonal(temp_corr.values, np.nan)\n",
    "    return(temp_corr)\n",
    "\n",
    "all_u_centcorr=correlations_centralities('all_u')\n",
    "knn_centcorr=correlations_centralities('knn')\n",
    "pos_w_centcorr=correlations_centralities('pos_w')\n",
    "\n",
    "fig,ax=plt.subplots(nrows=3, figsize=(8,16), sharex=True)\n",
    "ax=ax.flatten()\n",
    "for i in range(3):\n",
    "    tdata=[all_u_centcorr,knn_centcorr, pos_w_centcorr][i]\n",
    "    tname=['all_u','knn','pos_w'][i]\n",
    "    sns.heatmap(tdata, cmap=\"RdBu_r\", center=0, annot=True, ax=ax[i]);\n",
    "    ax[i].set(title=tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that most of the centrality metrics are positively correlated in the full network and in the positively coexpression network. However, this is not the case for the KNN network.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1. How do you explain the inverse relationship between degree and most other metrics in the KNN network?\n",
    "2. Why do you think that the KNN network specifically displays this opposite trend?\n",
    "\n",
    "The next figure may help in answering the question above. We highlight the most central nodes based on degree (red) and eccentricity (green), in addition to a random subset of 500 nodes. Of these, which do you think displays the shortest path to all other nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_centralities=full_centralities.loc[:,full_centralities.columns.str.contains('knn')]\n",
    "knn_centralities=knn_centralities.loc[:,knn_centralities.columns!='median_centrality|knn']\n",
    "knn_centralities.columns=knn_centralities.columns.str.replace('\\\\|.+','')\n",
    "\n",
    "##top nodes based on eccentricity\n",
    "knn_top_ecc=knn_centralities['eccentricity|knn'].sort_values(ascending=False).index.values[:1]\n",
    "\n",
    "##top nodes based on degree\n",
    "knn_top_deg=knn_centralities['degree|knn'].sort_values(ascending=False).index.values[:1]\n",
    "\n",
    "##random nodes to help visualize\n",
    "##warning, do not increase this value much higher than 500, or you may have problems rendering this image\n",
    "knn_others=knn_centralities.sample(500).index.values\n",
    "\n",
    "## full list\n",
    "node_list=np.append(np.append(knn_top_deg, knn_top_ecc), knn_others)\n",
    "\n",
    "## subsets nodes\n",
    "knn_to_draw=knn.subgraph(knn.vs.select([x.index for x in knn.vs if x['name'] in node_list]))\n",
    "knn_to_draw.vs['color']=['red' if x['name']  in knn_top_deg else 'green' if x['name'] in knn_top_ecc else 'white' for x in knn_to_draw.vs]\n",
    "\n",
    "layout = knn_to_draw.layout_auto()\n",
    "ig.plot(knn_to_draw, layout=layout, vertex_color=knn_to_draw.vs['color'], edge_color='silver', vertex_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node communities may be identified based on different metrics including [Modularity](https://en.wikipedia.org/wiki/Modularity_(networks)) or [Density](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.84.016114). We will look at community detection through modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity of a small graph\n",
    "\n",
    "Recall that the Modularity of a community is given by $$Q = \\frac{1}{2m} \\sum_{c}(e_c - \\frac{K_c^2}{2m})$$\n",
    "\n",
    "where $e_c$ is the number of edges in community $c$, and $\\frac{K_c^2}{2m}$ is the expected number of edges in the community given the $K_c$ sum of degrees of its nodes, for a network with $m$ edges. This will correspond to\n",
    "\n",
    "$$Q = \\frac{1}{2m} \\sum_{ij}[A_{ij} - \\frac{k_i k_j}{2m}\\delta(c_i,c_j)]$$\n",
    "\n",
    "where $A_ij$ is the Adjacency between nodes $i$ and $j$, $k_i$ and $k_j$ are their degree, and $\\delta(c_i,c_j)$ is the [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta), defined as 1 if nodes $i$ and $j$ are in the same community, or 0 if they are not. Let's examine the following small network, with communities given by the two colors: red `[A,B,C,D,H]` and blue `[E,F,G]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph([(0,1), (0,2), (1,2), (0,3), (2,3), (1,3), (2,4), (4,5), (5,6), (4,6), (4,7), (5,7), (6,7)])\n",
    "\n",
    "g.vs[\"name\"] = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\",\"H\"]\n",
    "g.vs[\"module\"] = [\"f\", \"f\", \"f\", \"f\", \"m\", \"m\",\"m\", \"f\"]\n",
    "\n",
    "\n",
    "layout = g.layout_circle()\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "color_dict = {\"m\": \"cyan\", \"f\": \"pink\"}\n",
    "g.vs[\"color\"] = [color_dict[module] for module in g.vs[\"module\"]]\n",
    "ig.plot(g, layout = layout, bbox = (300, 300), margin = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network, we have the following adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( g.get_adjacency(), index=g.vs[\"name\"],  columns=g.vs[\"name\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the modularity of this network given the 2 communities above, herein identified as communities `1` and `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(graph, membership):\n",
    "    \"\"\"\n",
    "    Computes the modularity of `graph` for a given module `membership`.\n",
    "    \"\"\"\n",
    "    m=len(graph.es.indices) #edge number\n",
    "    A=pd.DataFrame(graph.get_adjacency()) #adjacency matrix\n",
    "    Q=[] \n",
    "    for i in A.index:\n",
    "        for j in A.columns:\n",
    "            if(membership[i]==membership[j]):\n",
    "                deltaij=1\n",
    "            else:\n",
    "                deltaij=0\n",
    "            Q=np.append(Q, (A.loc[i,j]-(g.vs[i].degree()*g.vs[j].degree())/(2*m))*deltaij)\n",
    "    Q=(1/(2*m))*np.sum(Q)\n",
    "    out=Q\n",
    "    return(out)\n",
    "\n",
    "modularity(g, [1,1,1,1,2,2,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a very small network, we can take a brute-force approach and examine all possible membership combinations that will yield the highest possible modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memberships=[x for x in itertools.product([1,2], repeat=8)]\n",
    "\n",
    "\n",
    "membership_modularity=pd.DataFrame()\n",
    "for membership in all_memberships:\n",
    "    group1=[g.vs['name'][x] for x in range(len(g.vs['name'])) if membership[x]==1]\n",
    "    group2=[g.vs['name'][x] for x in range(len(g.vs['name'])) if membership[x]==2]\n",
    "    out=pd.Series([group1, group2, membership, modularity(g, membership)], index=['comm1', 'comm2','memb','Q'])\n",
    "    membership_modularity=pd.concat([membership_modularity, out], axis=1)\n",
    "    \n",
    "membership_modularity=membership_modularity.T.sort_values('Q', ascending=False)\n",
    "membership_modularity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 2 top hits that maximize modularity define the same communities as `[A,B,C,D]` and `[E,F,G,H]`. For larger networks we cannot use a brute-force approach, and instead rely on the 2-pass Louvain algorithm, which has since been improved with the Leiden algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity of gene-metabolite networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we perform the community analysis on the 4 networks. We will perform one additional community analysis by considering the edge weights from the positively associated network. Importantly, this method searches for the largest possible communities for our network, which may not always be the desired. Alternative models such as the [Constant Potts Model](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.84.016114) allow you to identify smaller communities. Should we know that our data has special feature classes, we can compare whether the communities identify those classes by examining them individually, and increasing the resolution if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comm = leidenalg.find_partition(pos_w, leidenalg.ModularityVertexPartition)\n",
    "pos_w_comm = leidenalg.find_partition(pos_w, leidenalg.ModularityVertexPartition, weights='w')\n",
    "all_comm = leidenalg.find_partition(all_u, leidenalg.ModularityVertexPartition)\n",
    "knn_comm = leidenalg.find_partition(knn, leidenalg.ModularityVertexPartition)\n",
    "random_all = leidenalg.find_partition(random_allu, leidenalg.ModularityVertexPartition)\n",
    "random_posw = leidenalg.find_partition(random_posw, leidenalg.ModularityVertexPartition)\n",
    "random_knn = leidenalg.find_partition(random_knn, leidenalg.ModularityVertexPartition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, we can see that the modularity score of any of the networks is substantially larger than that of the random network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pos_comm.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pos_w_comm.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(all_comm.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(knn_comm.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(random_all.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(random_posw.modularity,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(random_knn.modularity,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the different communities by size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Compiles feat lists per community\n",
    "def get_community_table():\n",
    "    comm_counts=pd.DataFrame()\n",
    "    feat_lists=pd.DataFrame()\n",
    "    for i in [0,1,2,3]:\n",
    "        graph=[pos_w,pos_w,all_u,knn][i]\n",
    "        comm=[pos_comm,pos_w_comm,all_comm,knn_comm][i]\n",
    "        name=['pos','pos_w','all','knn'][i]\n",
    "        temp=pd.DataFrame(list(zip(graph.vs['name'],[x+1 for x in comm.membership]))).rename(columns={0:'feat',1:'community'})\n",
    "        counts=pd.DataFrame(temp.groupby('community')['feat'].agg(len))\n",
    "        counts.columns=[name]\n",
    "        comm_counts=pd.concat([comm_counts, counts], axis=1)\n",
    "        \n",
    "        gl=pd.DataFrame(temp.groupby('community')['feat'].apply(list)).reset_index()\n",
    "        gl['community']=['c'+str(i) for i in gl['community']]\n",
    "        gl['network']=name\n",
    "        gl=gl.loc[:,['network','community','feat']]\n",
    "        feat_lists=pd.concat([feat_lists, gl])\n",
    "        \n",
    "    comm_counts.index=['c'+str(i) for i in comm_counts.index]\n",
    "    return([comm_counts,feat_lists])\n",
    "\n",
    "#Plotting community sizes\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "bar_data=get_community_table()[0].fillna(0).T\n",
    "bar_data.plot(kind='bar', stacked=True, ax=ax);\n",
    "# ## number of communities in each\n",
    "# for index, row in groupedvalues.iterrows():\n",
    "#     g.text(row.name,row.tip, round(row.total_bill,2), color='black', ha=\"center\")\n",
    "ax.legend(get_community_table()[0].index, loc='right', bbox_to_anchor=(1.15, 1));\n",
    "ax.set_title('Community size')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Some of the communities are very small in the `pos` and full networks, and comprise only 2 and 3 elements. Can we really call this a community?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.barplot(\n",
    "    data=bar_data.T.unstack().reset_index().rename(columns={'level_0':'network','level_1':'community',0:'size'}),\n",
    "    x='network',y='size', hue='community'\n",
    "           )\n",
    "\n",
    "ax.set(yscale='log');\n",
    "ax.legend(loc='right', bbox_to_anchor=(1.15, 1));\n",
    "ax.set_title('Community size')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional analysis**  \n",
    "In order to perform functional enrichment, we will extract each of the communities and perform a hypergeometric test **on the genes** to understand whether they are particularly enriched in specific biological functions.  \n",
    "We will use [enrichr](https://gseapy.readthedocs.io/en/master/gseapy_example.html#2.-Enrichr-Example) to perform the gene set enrichment analysis. As background we will use the full list of genes that were quantified.\n",
    "\n",
    "We will look at 3 gene set libraries. Should you have other kinds of data, enrichr allows you to define your own feature sets and perform a similar analysis. The challenge is in identifying comprehensive and well curated gene sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the available human libraries\n",
    "gp.get_library_name(organism='Human')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we will search 3 libraries for significantly enriched gene sets\n",
    "gene_sets=['GO_Biological_Process_2018','KEGG_2019_Human','OMIM_Disease']\n",
    "background=[x for x in all_u.copy().vs['name'] if x in data.loc[data.Type=='genes'].index]\n",
    "all_genes=data.loc[data.Type=='genes'].index\n",
    "\n",
    "def perform_enrich(network):\n",
    "    temp=get_community_table()[1].copy()\n",
    "    temp=temp.loc[temp['network']==network]\n",
    "    output_enrichr=pd.DataFrame()\n",
    "    for comm in temp['community'].values:\n",
    "        gl=list(temp.loc[temp['community']==comm, 'feat'])[0]\n",
    "        gl=list([x for x in gl if x in all_genes])\n",
    "        \n",
    "        if(len(gl)<30):\n",
    "            continue\n",
    "        print('Found '+str(len(gl))+' genes in community '+comm)\n",
    "        for bp in gene_sets:\n",
    "            print('Analyzing '+network+' network | Comm: '+comm+'/'+str(len(temp.index))+'  | BP: '+bp)\n",
    "            enr=gp.enrichr(\n",
    "                gene_list=gl,\n",
    "                gene_sets=bp,\n",
    "                background=background,\n",
    "                outdir='Enrichr',\n",
    "                format='png',\n",
    "                cutoff=1\n",
    "            )\n",
    "\n",
    "            results=enr.results.sort_values('Adjusted P-value', ascending=True)\n",
    "            results=results.loc[results['Adjusted P-value']<0.05,]\n",
    "            results['BP']=bp\n",
    "            results['Comm']=comm\n",
    "            results['Graph']=network\n",
    "            output_enrichr=pd.concat([output_enrichr, results])\n",
    "            \n",
    "    return(output_enrichr)\n",
    "\n",
    "all_enriched=pd.DataFrame()\n",
    "for net in ['pos', 'pos_w', 'all', 'knn']: \n",
    "    all_enriched=pd.concat([all_enriched,perform_enrich(net)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of the cell above you can readily see that some communities such as `c1` display no enriched terms from either [GO](http://geneontology.org/), [KEGG](https://www.genome.jp/kegg/) or [OMIM](https://www.omim.org/).\n",
    "\n",
    "Running the command above not only gives you the results after significance testing (Q<0.05), but it also outputs some preliminary barplots with the statistically significant results (found under `/Enrichr/`). For instance:\n",
    "<img src=\"./Enrichr/GO_Biological_Process_2018.human.enrichr.reports.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enriched_terms=all_enriched.loc[:,['Graph','Comm','Term','Adjusted P-value']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_terms['Adjusted P-value']=-1*np.log10(enriched_terms['Adjusted P-value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "data_bars=pd.DataFrame(enriched_terms.groupby(['Graph','Comm'])['Term'].agg('count')).stack().reset_index().rename(columns={0:'Count'})\n",
    "sns.barplot(x='Graph', y='Count', data=data_bars, hue='Comm')\n",
    "ax.set_title('Number of significant Terms (Q < 0.05) per community')\n",
    "ax.legend(loc='right', bbox_to_anchor=(1.15, 1));\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of these communities are very big, which explains the big number of biological processes found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Number of genes/community\n",
    "# We skipped communities with <30 genes\n",
    "get_community_table()[0].fillna(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(enriched_terms.groupby(['Graph','Comm'])['Term'].agg('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find whether the Full Network, Positively associated, and Positively associated weighted, show any common terms among their biggest communities. We do not compare with kNN-G as this shows very homogeneous and different communities than the other two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding consensus\n",
    "temp=enriched_terms.copy()\n",
    "temp['comm_term']=temp.Comm+'_'+temp.Term\n",
    "temp=temp.loc[:,['Graph','comm_term']]\n",
    "\n",
    "consensus=pd.DataFrame()\n",
    "consensus=pd.concat([consensus, temp.loc[temp['Graph']=='pos']])\n",
    "consensus=pd.merge(consensus,\n",
    "                   temp.loc[temp['Graph']=='pos_w'], on=\"comm_term\", how='outer', suffixes=['pos','pos_w'])\n",
    "consensus=pd.merge(consensus, \n",
    "                   temp.loc[temp['Graph']=='all'], on=\"comm_term\", how='outer').rename(columns={'Graph':'all'})\n",
    "\n",
    "consensus=consensus.loc[consensus.isna().sum(1)==0].loc[:,['comm_term','Graphpos','Graphpos_w','all']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the biggest communities we find several biological processes (55) that are simultaneously identified in the same community in the three graphs (`Full`, `Pos assoc`, and `Pos assoc weighted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus['comm']=[x[0] for x in consensus.comm_term.str.split('\\_')]\n",
    "consensus.groupby('comm')['comm_term'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**  \n",
    "- Would you exclude any communities based on its size?\n",
    "- Having identified these communities, how would you try to validate them?\n",
    "- Would you now determine the relevant community to investigate further?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to export communities to use them in other sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires running the community detection from the previous section\n",
    "patlas=pd.read_csv('data/proteinatlas.tsv', sep=\"\\t\").loc[:,['Ensembl','Gene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Compiles gene lists per community. We need Ensembl ids for further analyses\n",
    "def get_ensembl():\n",
    "    comm_counts=pd.DataFrame()\n",
    "    gene_lists=pd.DataFrame()\n",
    "    for i in [0,1,2,3]:\n",
    "        graph=[pos_w,pos_w,all_u,knn][i]\n",
    "        comm=[pos_comm,pos_w_comm,all_comm,knn_comm][i]\n",
    "        name=['pos','pos_w','all','knn'][i]\n",
    "        temp=pd.DataFrame(list(zip(graph.vs['name'],[x+1 for x in comm.membership]))).rename(columns={0:'gene',1:'community'})\n",
    "\n",
    "        gl=pd.DataFrame(temp.groupby('community')['gene'].apply(list)).reset_index()\n",
    "        gl['community']=['c'+str(i) for i in gl['community']]\n",
    "        gl['network']=name\n",
    "        gl=gl.loc[:,['network','community','gene']]\n",
    "        gene_lists=pd.concat([gene_lists, gl])\n",
    "\n",
    "    gene_communities=gene_lists\n",
    "    gene_mat=pd.DataFrame()\n",
    "    for net in gene_communities['network'].unique():\n",
    "        temp=gene_communities.copy().loc[gene_communities['network']==net,]\n",
    "        for comm in temp['community'].unique():\n",
    "            gl=list(temp.copy().loc[temp['community']==comm,'gene'])[0]\n",
    "            el=[patlas.loc[patlas['Gene']==x,'Ensembl'].iloc[0] for x in gl if x in patlas['Gene'].values]\n",
    "\n",
    "            df=pd.DataFrame([net,comm,gl,el]).T\n",
    "            df.columns=['network','community','Gene','Ensembl']\n",
    "            gene_mat=pd.concat([gene_mat, df])\n",
    "                \n",
    "    return(gene_mat)\n",
    "\n",
    "get_ensembl().to_csv('data/gene_communities.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've performed some network analyses based on a met-gene association network. We've explored different centrality measures to characterize the networks, and identified the communities of genes in these networks. We have also used gene set enrichment analysis to characterize these communities based on the genes, but it remains to show whether similar results would be attained if we considered the metabolites in each community.  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
